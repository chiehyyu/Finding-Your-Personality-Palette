{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GRU.ipynb","provenance":[],"mount_file_id":"1QEYuaAAQwYqkJ7obvCvuAzsw5ncjQpcA","authorship_tag":"ABX9TyMkdfMTj9l3IBMI1XgJUfAD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uD0rsJ5KcipB","outputId":"47d39829-f5d7-4160-dd32-34708379bf7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 35, 100)           1000000   \n","                                                                 \n"," dropout_3 (Dropout)         (None, 35, 100)           0         \n","                                                                 \n"," gru_1 (GRU)                 (None, 64)                31872     \n","                                                                 \n"," dropout_4 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                4160      \n","                                                                 \n"," dropout_5 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 5)                 325       \n","                                                                 \n","=================================================================\n","Total params: 1,036,357\n","Trainable params: 1,036,357\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","49/49 [==============================] - ETA: 0s - loss: 2.2328 - accuracy: 0.4790\n","Epoch 1: val_loss improved from inf to 1.00486, saving model to /content/drive/MyDrive/final_project/weights_GRU.h5\n","49/49 [==============================] - 8s 122ms/step - loss: 2.2328 - accuracy: 0.4790 - val_loss: 1.0049 - val_accuracy: 0.7450\n","Epoch 2/10\n","49/49 [==============================] - ETA: 0s - loss: 0.7406 - accuracy: 0.8365\n","Epoch 2: val_loss improved from 1.00486 to 0.59076, saving model to /content/drive/MyDrive/final_project/weights_GRU.h5\n","49/49 [==============================] - 5s 107ms/step - loss: 0.7406 - accuracy: 0.8365 - val_loss: 0.5908 - val_accuracy: 0.8797\n","Epoch 3/10\n","49/49 [==============================] - ETA: 0s - loss: 0.5373 - accuracy: 0.9002\n","Epoch 3: val_loss improved from 0.59076 to 0.55599, saving model to /content/drive/MyDrive/final_project/weights_GRU.h5\n","49/49 [==============================] - 5s 108ms/step - loss: 0.5373 - accuracy: 0.9002 - val_loss: 0.5560 - val_accuracy: 0.8984\n","Epoch 4/10\n","49/49 [==============================] - ETA: 0s - loss: 0.4716 - accuracy: 0.9241\n","Epoch 4: val_loss did not improve from 0.55599\n","49/49 [==============================] - 5s 107ms/step - loss: 0.4716 - accuracy: 0.9241 - val_loss: 0.5622 - val_accuracy: 0.8876\n","Epoch 5/10\n","49/49 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.9418\n","Epoch 5: val_loss improved from 0.55599 to 0.51760, saving model to /content/drive/MyDrive/final_project/weights_GRU.h5\n","49/49 [==============================] - 5s 108ms/step - loss: 0.4192 - accuracy: 0.9418 - val_loss: 0.5176 - val_accuracy: 0.9006\n","Epoch 6/10\n","49/49 [==============================] - ETA: 0s - loss: 0.4064 - accuracy: 0.9453\n","Epoch 6: val_loss did not improve from 0.51760\n","49/49 [==============================] - 5s 107ms/step - loss: 0.4064 - accuracy: 0.9453 - val_loss: 0.5445 - val_accuracy: 0.8934\n","Epoch 7/10\n","38/49 [======================>.......] - ETA: 1s - loss: 0.3628 - accuracy: 0.9554"]}],"source":["import numpy as np\n","import pandas as pd \n","import tensorflow as tf\n","import nltk\n","from nltk.stem.porter import PorterStemmer\n","import re\n","import os\n","\n","from tensorflow.keras.layers import Embedding, Dense, GRU, LSTM, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from nltk.corpus import stopwords\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","import time\n","try:\n","    os.remove('/content/drive/MyDrive/final_project/weights_GRU.h5')\n","except OSError:\n","    pass\n","start = time.time()\n","df = pd.read_csv('/content/drive/MyDrive/final_project/Emotion_project.csv') # read the dataset\n","df = df.dropna() # drop columns with NA values\n","X = df.drop('Emotion',axis=1) # input\n","y = df['Emotion'] # output\n","\n","vocab_size = 10000\n","messages = X.copy() # copy of output\n","messages.reset_index(inplace=True)\n","\n","nltk.download('stopwords')\n","# stopwords: frequent words in text.('the', 'and', 'I', etc.) They don't add much meaning to a sentence\n","ps = PorterStemmer()\n","corpus = []\n","for i in range(0, len(messages)):\n","    review = re.sub('[^a-zA-Z]', ' ', messages['Text'][i]) # remove special characters\n","    review = review.lower() # turn into lower case\n","    review = review.split() \n","    review = [ps.stem(word) for word in review if (word == 'not' or word == 'no' or word == 'nor' or not word in stopwords.words('english')) ] # remove stopwords\n","    review = ' '.join(review)\n","    corpus.append(review)\n","\n","onehot_repr = [one_hot(text,vocab_size)for text in corpus] # use one_hot encoding\n","\n","maxlength = 0\n","for x in corpus:\n","    maxlength = max(maxlength, len(x.split(' '))) # find text with max length\n","embedded_docs = pad_sequences(onehot_repr,padding = 'pre',maxlen = maxlength) # pad sequences to the same length\n","\n","X_final = np.array(embedded_docs)\n","label_encoder = preprocessing.LabelEncoder() \n","y_final = label_encoder.fit_transform(y) # encoding the target outputs to integers\n","y_final = np.array(y_final)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.3, random_state=42) # train-test split\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=21)  # train-validation split\n","\n","# create model\n","embedding_vector_features = 100\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_vector_features, input_length = maxlength))\n","# 降維: turn \"One hot representation\" to \"Distributed representation\"(represent the relation between words)\n","model.add(Dropout(0.3)) # randomly sets input units to 0 -> helps prevent overfitting\n","model.add(GRU(64)) # 64: dimensionality of the output space (output 維度)\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu', kernel_regularizer = tf.keras.regularizers.l1(0.01))) # output = activation(dot(input, kernel)+bias)\n","model.add(Dropout(0.3))\n","model.add(Dense(5, activation='softmax')) # 5 categories\n","model.compile(loss='sparse_categorical_crossentropy', optimizer= tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\n","model.summary()\n","\n","model_save = ModelCheckpoint('/content/drive/MyDrive/final_project/weights_GRU.h5', save_best_only = True, save_weights_only = True, monitor = 'val_loss', mode = 'min', verbose = 1)\n","# save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved\n","history = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs = 10, batch_size = 256, callbacks = [model_save])\n","#training process, record epoch, loss, accuracy...\n","# batch_size : number of samples per gradient update (一次丟多少資料進去)\n","# epochs : number of iteration ( 1 iteration represents going though whole training data)\n","# validation : check if the data is overfitting\n","model.save('/content/drive/MyDrive/final_project/weights_GRU.h5')\n","model.load_weights('/content/drive/MyDrive/final_project/weights_GRU.h5')\n","\n","end = time.time()\n","print(\"Process time: \",end - start)\n","\n","y_pred=np.argmax(model.predict(X_test), axis=-1) # predict test data\n","print(\"GRU Accuracy: \", accuracy_score(y_test,y_pred)) # calculate accuracy\n","\n"]},{"cell_type":"code","source":["def predict_emotion(stri):\n","    review = re.sub('[^a-zA-Z]', ' ', stri)\n","    review = review.lower()\n","    review = review.split()\n","    review = [ps.stem(word) for word in review if (word == 'not' or word == 'no' or word == 'nor' or not word in stopwords.words('english')) ]\n","    review = ' '.join(review)\n","    onehot_repr = [one_hot(review,vocab_size)] \n","    embed = pad_sequences(onehot_repr,padding='pre',maxlen=maxlength)\n","    predicti = model.predict(embed)\n","    print(predicti)\n","    print(\"Label: \",label_encoder.classes_[np.argmax(predicti)])\n","    #transform(predicti[0],stri)\n","\n","text = input(\"Input any text: \")\n","predict_emotion(text)"],"metadata":{"id":"Vb8_0QDEdtM_"},"execution_count":null,"outputs":[]}]}