{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"mount_file_id":"1n9RXCaRKI2g0QJeLoAocWGbsk5TGtJ2q","authorship_tag":"ABX9TyO+4AFzMt4MtZ/7h28wLC9O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCMIljj8cKgK","executionInfo":{"status":"ok","timestamp":1655113862947,"user_tz":-480,"elapsed":139882,"user":{"displayName":"陳存佩","userId":"12207295053803165369"}},"outputId":"755958ac-8661-4a42-be6c-d526a30d5b5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 35, 100)           1000000   \n","                                                                 \n"," dropout (Dropout)           (None, 35, 100)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 64)                42240     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense (Dense)               (None, 64)                4160      \n","                                                                 \n"," dropout_2 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 325       \n","                                                                 \n","=================================================================\n","Total params: 1,046,725\n","Trainable params: 1,046,725\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","49/49 [==============================] - ETA: 0s - loss: 2.3215 - accuracy: 0.3984\n","Epoch 1: val_loss improved from inf to 1.30871, saving model to /content/drive/MyDrive/final_project/weights_LSTM.h5\n","49/49 [==============================] - 9s 145ms/step - loss: 2.3215 - accuracy: 0.3984 - val_loss: 1.3087 - val_accuracy: 0.5245\n","Epoch 2/10\n","49/49 [==============================] - ETA: 0s - loss: 0.9896 - accuracy: 0.6736\n","Epoch 2: val_loss improved from 1.30871 to 0.83528, saving model to /content/drive/MyDrive/final_project/weights_LSTM.h5\n","49/49 [==============================] - 6s 122ms/step - loss: 0.9896 - accuracy: 0.6736 - val_loss: 0.8353 - val_accuracy: 0.7277\n","Epoch 3/10\n","49/49 [==============================] - ETA: 0s - loss: 0.7423 - accuracy: 0.7614\n","Epoch 3: val_loss improved from 0.83528 to 0.78727, saving model to /content/drive/MyDrive/final_project/weights_LSTM.h5\n","49/49 [==============================] - 6s 132ms/step - loss: 0.7423 - accuracy: 0.7614 - val_loss: 0.7873 - val_accuracy: 0.7320\n","Epoch 4/10\n","49/49 [==============================] - ETA: 0s - loss: 0.6626 - accuracy: 0.7833\n","Epoch 4: val_loss did not improve from 0.78727\n","49/49 [==============================] - 7s 142ms/step - loss: 0.6626 - accuracy: 0.7833 - val_loss: 0.7936 - val_accuracy: 0.7572\n","Epoch 5/10\n","49/49 [==============================] - ETA: 0s - loss: 0.5872 - accuracy: 0.8161\n","Epoch 5: val_loss improved from 0.78727 to 0.76706, saving model to /content/drive/MyDrive/final_project/weights_LSTM.h5\n","49/49 [==============================] - 6s 127ms/step - loss: 0.5872 - accuracy: 0.8161 - val_loss: 0.7671 - val_accuracy: 0.7702\n","Epoch 6/10\n","49/49 [==============================] - ETA: 0s - loss: 0.5415 - accuracy: 0.8421\n","Epoch 6: val_loss improved from 0.76706 to 0.74263, saving model to /content/drive/MyDrive/final_project/weights_LSTM.h5\n","49/49 [==============================] - 6s 127ms/step - loss: 0.5415 - accuracy: 0.8421 - val_loss: 0.7426 - val_accuracy: 0.8357\n","Epoch 7/10\n","49/49 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.9141\n","Epoch 7: val_loss improved from 0.74263 to 0.67635, saving model to /content/drive/MyDrive/final_project/weights_LSTM.h5\n","49/49 [==============================] - 7s 134ms/step - loss: 0.4612 - accuracy: 0.9141 - val_loss: 0.6764 - val_accuracy: 0.8595\n","Epoch 8/10\n","49/49 [==============================] - ETA: 0s - loss: 0.3976 - accuracy: 0.9398\n","Epoch 8: val_loss did not improve from 0.67635\n","49/49 [==============================] - 6s 121ms/step - loss: 0.3976 - accuracy: 0.9398 - val_loss: 0.6812 - val_accuracy: 0.8674\n","Epoch 9/10\n","49/49 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.9465\n","Epoch 9: val_loss did not improve from 0.67635\n","49/49 [==============================] - 6s 121ms/step - loss: 0.3769 - accuracy: 0.9465 - val_loss: 0.7001 - val_accuracy: 0.8617\n","Epoch 10/10\n","49/49 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.9526\n","Epoch 10: val_loss did not improve from 0.67635\n","49/49 [==============================] - 6s 121ms/step - loss: 0.3517 - accuracy: 0.9526 - val_loss: 0.7217 - val_accuracy: 0.8501\n","Process time:  135.09341168403625\n","LSTM Accuracy:  0.8745375042045073\n"]}],"source":["import numpy as np\n","import pandas as pd \n","import tensorflow as tf\n","import nltk\n","from nltk.stem.porter import PorterStemmer\n","import re\n","import os\n","\n","from tensorflow.keras.layers import Embedding, Dense, GRU, LSTM, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from nltk.corpus import stopwords\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","import time\n","try:\n","    os.remove('/content/drive/MyDrive/final_project/weights_LSTM.h5')\n","except OSError:\n","    pass\n","\n","start = time.time()\n","df = pd.read_csv('/content/drive/MyDrive/final_project/Emotion_project.csv') # read the dataset\n","df = df.dropna() # drop columns with NA values\n","X = df.drop('Emotion',axis=1) # input\n","y = df['Emotion'] # output\n","\n","vocab_size = 10000\n","messages = X.copy() # copy of output\n","messages.reset_index(inplace=True)\n","\n","nltk.download('stopwords')\n","# stopwords: frequent words in text.('the', 'and', 'I', etc.) They don't add much meaning to a sentence\n","ps = PorterStemmer()\n","corpus = []\n","for i in range(0, len(messages)):\n","    review = re.sub('[^a-zA-Z]', ' ', messages['Text'][i]) # remove special characters\n","    review = review.lower() # turn into lower case\n","    review = review.split() \n","    review = [ps.stem(word) for word in review if (word == 'not' or word == 'no' or word == 'nor' or not word in stopwords.words('english')) ] # remove stopwords\n","    review = ' '.join(review)\n","    corpus.append(review)\n","\n","onehot_repr = [one_hot(text,vocab_size)for text in corpus] # use one_hot encoding\n","\n","maxlength = 0\n","for x in corpus:\n","    maxlength = max(maxlength, len(x.split(' '))) # find text with max length\n","embedded_docs = pad_sequences(onehot_repr,padding = 'pre',maxlen = maxlength) # pad sequences to the same length\n","\n","X_final = np.array(embedded_docs)\n","label_encoder = preprocessing.LabelEncoder() \n","y_final = label_encoder.fit_transform(y) # encoding the target outputs to integers\n","y_final = np.array(y_final)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.3, random_state=42) # train-test split\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=21)  # train-validation split\n","\n","# create model\n","embedding_vector_features = 100\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_vector_features, input_length = maxlength))\n","# 降維: turn \"One hot representation\" to \"Distributed representation\"(represent the relation between words)\n","model.add(Dropout(0.3)) # randomly sets input units to 0 -> helps prevent overfitting\n","model.add(LSTM(64)) # 64: dimensionality of the output space (output 維度)\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu', kernel_regularizer = tf.keras.regularizers.l1(0.01))) # output = activation(dot(input, kernel)+bias)\n","model.add(Dropout(0.3))\n","model.add(Dense(5, activation='softmax')) # 5 categories\n","model.compile(loss='sparse_categorical_crossentropy', optimizer= tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\n","model.summary()\n","\n","model_save = ModelCheckpoint('/content/drive/MyDrive/final_project/weights_LSTM.h5', save_best_only = True, save_weights_only = True, monitor = 'val_loss', mode = 'min', verbose = 1)\n","# save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved\n","history = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs = 10, batch_size = 256, callbacks = [model_save])\n","#training process, record epoch, loss, accuracy...\n","# batch_size : number of samples per gradient update (一次丟多少資料進去)\n","# epochs : number of iteration ( 1 iteration represents going though whole training data)\n","# validation : check if the data is overfitting\n","model.save('/content/drive/MyDrive/final_project/weights_LSTM.h5')\n","model.load_weights('/content/drive/MyDrive/final_project/weights_LSTM.h5')\n","\n","end = time.time()\n","print(\"Process time: \",end - start)\n","\n","y_pred=np.argmax(model.predict(X_test), axis=-1) # predict test data\n","print(\"LSTM Accuracy: \", accuracy_score(y_test,y_pred)) # calculate accuracy"]},{"cell_type":"code","source":["def predict_emotion(stri):\n","    review = re.sub('[^a-zA-Z]', ' ', stri)\n","    review = review.lower()\n","    review = review.split()\n","    review = [ps.stem(word) for word in review if (word == 'not' or word == 'no' or word == 'nor' or not word in stopwords.words('english')) ]\n","    review = ' '.join(review)\n","    onehot_repr = [one_hot(review,vocab_size)] \n","    embed = pad_sequences(onehot_repr,padding='pre',maxlen=maxlength)\n","    predicti = model.predict(embed)\n","    print(predicti)\n","    print(\"Label: \",label_encoder.classes_[np.argmax(predicti)])\n","    #transform(predicti[0],stri)\n","\n","text = input(\"Input any text: \")\n","predict_emotion(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UleyXUmadwh1","executionInfo":{"status":"ok","timestamp":1655113887517,"user_tz":-480,"elapsed":9048,"user":{"displayName":"陳存佩","userId":"12207295053803165369"}},"outputId":"4f828918-6d50-4e15-c134-8db06b1fc71e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Input any text: i love this world, wonderful\n","[[0.10042729 0.07277312 0.65159464 0.15637462 0.01883036]]\n","Label:  happy\n"]}]}]}