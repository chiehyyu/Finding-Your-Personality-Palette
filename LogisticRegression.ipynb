{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LogisticRegression.ipynb","provenance":[],"mount_file_id":"1wS8vljVT70ViJzlpiBrWN-O7pkTROpxm","authorship_tag":"ABX9TyNF3+rRWgOjGbgFwc27ZrLs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wzYUqBWGeIrs","executionInfo":{"status":"ok","timestamp":1655114173730,"user_tz":-480,"elapsed":6205,"user":{"displayName":"陳存佩","userId":"12207295053803165369"}},"outputId":"27919488-c7cb-426e-f407-9014fe08f11a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"stream","name":"stdout","text":["Process time:  0.686943531036377\n","LR Accuracy:  0.8301379078372014\n","Input any text: i love you\n","Label:  ['happy']\n","[[0.12136831 0.26910192 0.36296079 0.21247255 0.03409643]]\n"]}],"source":["import pandas as pd\n","import neattext.functions as nfs\n","from sklearn.linear_model import LogisticRegression\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","\n","import time\n","\n","start = time.time()\n","df=pd.read_csv(r\"/content/drive/MyDrive/final_project/Emotion_project.csv\")\n","df = df.dropna() # drop columns with NA values\n","df[\"clean_text\"] = df[\"Text\"].apply(nfs.remove_stopwords) # User handles and remove Stopwords\n","\n","X = df[\"clean_text\"] # Extract two Features from the data\n","y = df[\"Emotion\"]\n","\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100) # Let's split the data into two parts\n","\n","lr = Pipeline(steps=[(\"cv\",CountVectorizer()),(\"lr\",LogisticRegression(max_iter=10))]) # Create a Model on LogisticRegression\n","lr.fit(X_train,y_train)\n","\n","end = time.time()\n","print(\"Process time: \",end - start)\n","\n","print(\"LR Accuracy: \",lr.score(X_test,y_test)) # Show the Score of LogisticRegression Model\n","\n","text = input(\"Input any text: \")\n","label = lr.predict([text])\n","prob = lr.predict_proba([text])\n","print(\"Label: \", label)\n","print(prob)"]},{"cell_type":"code","source":["import matplotlib.colors as mcolors\n","import numpy as np\n","\n","!pip3 install ColabTurtle\n","from ColabTurtle.Turtle import *\n","\n","def secondmax(list):\n","  sublist = [x for x in list if x < list.max()]\n","  indices = np.where(list == max(sublist))\n","  return max(sublist), indices[0][0]\n","\n","def transform(list, text) :\n","    # anger / fear / happy / sadness / surprize\n","    emotions = [[193, 73, 83], [131, 140, 111], [232, 209, 155], [36, 128, 190], [228, 176, 165]]\n","    rgb = []\n","    for i in range(0, 3):\n","        weight = 0\n","        for j in range(0, 5):\n","            alpha = 1\n","            if list[j] == max(list): alpha = 1.2  # first\n","            elif list[j] == secondmax(list)[0]: alpha = 1.2  # second\n","            elif list[j] == min(list): alpha = 0.8  # last\n","            weight += emotions[j][i] * list[j] * alpha\n","        weight /= 255\n","        if weight > 1: weight = 1\n","        rgb.append(weight)\n","\n","    (h, s, v) = mcolors.rgb_to_hsv(rgb)      \n","    light = mcolors.hsv_to_rgb([h, 0.05, 0.9])\n","    dark = mcolors.hsv_to_rgb([h, s, 0.5])\n","\n","    percentages = []\n","    for i in list :\n","        i *= 1000\n","        i = int(i)\n","        i /= 10\n","        percentages.append(' ' + str(i) + ' %')\n","    color_dict = {\n","        list[0]: \"Anger\",\n","        list[1]: \"Fear\",\n","        list[2]: \"Happy\",\n","        list[3]: \"Sadness\",\n","        list[4]: \"Surprize\"\n","    }\n","    first = list.argmax()\n","    second = secondmax(list)[1]\n","\n","    initializeTurtle()\n","    penup()\n","    hideturtle()\n","    bgcolor(\"#FFFFFF\")\n","    color(mcolors.to_hex(dark))\n","    style = (20, 'Verdana', 'italic')\n","    write(text, align='center', font=style)\n","    backward(50)\n","    color(mcolors.to_hex(rgb))\n","    style = (20, 'Verdana', 'bold')\n","    write(color_dict[list[first]] + percentages[first] + ' + ' + color_dict[list[second]] + percentages[second], align='center', font=style)\n","\n","transform(prob[0],text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"id":"h24pBCSxfThv","executionInfo":{"status":"ok","timestamp":1655114263997,"user_tz":-480,"elapsed":7964,"user":{"displayName":"陳存佩","userId":"12207295053803165369"}},"outputId":"8a324292-8109-4c11-a0d1-cc479328e6b0"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ColabTurtle in /usr/local/lib/python3.7/dist-packages (2.1.0)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","      <svg width=\"800\" height=\"500\">\n","        <rect width=\"100%\" height=\"100%\" fill=\"#ffffff\"/>\n","        <text x=\"400\" y=\"250\" fill=\"#807d70\" text-anchor=\"middle\" style=\"font-size:20px;font-family:'Verdana';font-style:italic;\">i love you</text><text x=\"400.0\" y=\"300.0\" fill=\"#b5b19e\" text-anchor=\"middle\" style=\"font-size:20px;font-family:'Verdana';font-weight:bold;\">Happy 36.2 % + Fear 26.9 %</text>\n","        <g visibility=hidden transform=\"rotate(360,400.0,300.0) translate(382.0, 282.0)\">\n","<path style=\" stroke:none;fill-rule:evenodd;fill:#b5b19e;fill-opacity:1;\" d=\"M 18.214844 0.632812 C 16.109375 1.800781 15.011719 4.074219 15.074219 7.132812 L 15.085938 7.652344 L 14.785156 7.496094 C 13.476562 6.824219 11.957031 6.671875 10.40625 7.066406 C 8.46875 7.550781 6.515625 9.15625 4.394531 11.992188 C 3.0625 13.777344 2.679688 14.636719 3.042969 15.027344 L 3.15625 15.152344 L 3.519531 15.152344 C 4.238281 15.152344 4.828125 14.886719 8.1875 13.039062 C 9.386719 12.378906 10.371094 11.839844 10.378906 11.839844 C 10.386719 11.839844 10.355469 11.929688 10.304688 12.035156 C 9.832031 13.09375 9.257812 14.820312 8.96875 16.078125 C 7.914062 20.652344 8.617188 24.53125 11.070312 27.660156 C 11.351562 28.015625 11.363281 27.914062 10.972656 28.382812 C 8.925781 30.84375 7.945312 33.28125 8.238281 35.1875 C 8.289062 35.527344 8.28125 35.523438 8.917969 35.523438 C 10.941406 35.523438 13.074219 34.207031 15.136719 31.6875 C 15.359375 31.417969 15.328125 31.425781 15.5625 31.574219 C 16.292969 32.042969 18.023438 32.964844 18.175781 32.964844 C 18.335938 32.964844 19.941406 32.210938 20.828125 31.71875 C 20.996094 31.625 21.136719 31.554688 21.136719 31.558594 C 21.203125 31.664062 21.898438 32.414062 22.222656 32.730469 C 23.835938 34.300781 25.5625 35.132812 27.582031 35.300781 C 27.90625 35.328125 27.9375 35.308594 28.007812 34.984375 C 28.382812 33.242188 27.625 30.925781 25.863281 28.425781 L 25.542969 27.96875 L 25.699219 27.785156 C 28.945312 23.960938 29.132812 18.699219 26.257812 11.96875 L 26.207031 11.84375 L 27.945312 12.703125 C 31.53125 14.476562 32.316406 14.800781 33.03125 14.800781 C 33.976562 14.800781 33.78125 13.9375 32.472656 12.292969 C 28.519531 7.355469 25.394531 5.925781 21.921875 7.472656 L 21.558594 7.636719 L 21.578125 7.542969 C 21.699219 6.992188 21.761719 5.742188 21.699219 5.164062 C 21.496094 3.296875 20.664062 1.964844 19.003906 0.855469 C 18.480469 0.503906 18.457031 0.5 18.214844 0.632812\"/>\n","</g>\n","      </svg>\n","    "]},"metadata":{}}]}]}