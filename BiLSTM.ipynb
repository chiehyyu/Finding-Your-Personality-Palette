{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BiLSTM.ipynb","provenance":[],"mount_file_id":"1KNUPKeorNXl8go5jAL8UQaYx_XIyxzL-","authorship_tag":"ABX9TyOx/WeUzyEakSYscYBfKQaQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbjUSB7LOZFg","executionInfo":{"status":"ok","timestamp":1655109720932,"user_tz":-480,"elapsed":12016,"user":{"displayName":"陳存佩","userId":"12207295053803165369"}},"outputId":"75338819-6e74-49e5-e2d0-4aeeb6c4d085"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"970br8_-ktjN","executionInfo":{"status":"ok","timestamp":1655113572423,"user_tz":-480,"elapsed":276922,"user":{"displayName":"陳存佩","userId":"12207295053803165369"}},"outputId":"f5478b3b-8399-4cd3-a65c-3873c8ab4901"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_3 (Embedding)     (None, 35, 100)           1000000   \n","                                                                 \n"," dropout_9 (Dropout)         (None, 35, 100)           0         \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 128)              84480     \n"," nal)                                                            \n","                                                                 \n"," dropout_10 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_6 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dropout_11 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_7 (Dense)             (None, 5)                 325       \n","                                                                 \n","=================================================================\n","Total params: 1,093,061\n","Trainable params: 1,093,061\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","49/49 [==============================] - ETA: 0s - loss: 2.8081 - accuracy: 0.3544\n","Epoch 1: val_loss improved from inf to 1.42353, saving model to /content/drive/MyDrive/final_project/weights_BiLSTM.h5\n","49/49 [==============================] - 18s 285ms/step - loss: 2.8081 - accuracy: 0.3544 - val_loss: 1.4235 - val_accuracy: 0.3617\n","Epoch 2/10\n","49/49 [==============================] - ETA: 0s - loss: 1.1873 - accuracy: 0.5671\n","Epoch 2: val_loss improved from 1.42353 to 0.97132, saving model to /content/drive/MyDrive/final_project/weights_BiLSTM.h5\n","49/49 [==============================] - 14s 276ms/step - loss: 1.1873 - accuracy: 0.5671 - val_loss: 0.9713 - val_accuracy: 0.7197\n","Epoch 3/10\n","49/49 [==============================] - ETA: 0s - loss: 0.8300 - accuracy: 0.7541\n","Epoch 3: val_loss improved from 0.97132 to 0.82901, saving model to /content/drive/MyDrive/final_project/weights_BiLSTM.h5\n","49/49 [==============================] - 16s 318ms/step - loss: 0.8300 - accuracy: 0.7541 - val_loss: 0.8290 - val_accuracy: 0.7442\n","Epoch 4/10\n","49/49 [==============================] - ETA: 0s - loss: 0.7232 - accuracy: 0.7829\n","Epoch 4: val_loss did not improve from 0.82901\n","49/49 [==============================] - 12s 252ms/step - loss: 0.7232 - accuracy: 0.7829 - val_loss: 0.8558 - val_accuracy: 0.7255\n","Epoch 5/10\n","49/49 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.7944\n","Epoch 5: val_loss improved from 0.82901 to 0.82512, saving model to /content/drive/MyDrive/final_project/weights_BiLSTM.h5\n","49/49 [==============================] - 13s 263ms/step - loss: 0.6751 - accuracy: 0.7944 - val_loss: 0.8251 - val_accuracy: 0.7493\n","Epoch 6/10\n","49/49 [==============================] - ETA: 0s - loss: 0.6292 - accuracy: 0.8300\n","Epoch 6: val_loss improved from 0.82512 to 0.74295, saving model to /content/drive/MyDrive/final_project/weights_BiLSTM.h5\n","49/49 [==============================] - 13s 266ms/step - loss: 0.6292 - accuracy: 0.8300 - val_loss: 0.7430 - val_accuracy: 0.8372\n","Epoch 7/10\n","49/49 [==============================] - ETA: 0s - loss: 0.5383 - accuracy: 0.9064\n","Epoch 7: val_loss improved from 0.74295 to 0.73953, saving model to /content/drive/MyDrive/final_project/weights_BiLSTM.h5\n","49/49 [==============================] - 14s 289ms/step - loss: 0.5383 - accuracy: 0.9064 - val_loss: 0.7395 - val_accuracy: 0.8653\n","Epoch 8/10\n","49/49 [==============================] - ETA: 0s - loss: 0.4663 - accuracy: 0.9365\n","Epoch 8: val_loss improved from 0.73953 to 0.68721, saving model to /content/drive/MyDrive/final_project/weights_BiLSTM.h5\n","49/49 [==============================] - 13s 266ms/step - loss: 0.4663 - accuracy: 0.9365 - val_loss: 0.6872 - val_accuracy: 0.8696\n","Epoch 9/10\n","49/49 [==============================] - ETA: 0s - loss: 0.4155 - accuracy: 0.9496\n","Epoch 9: val_loss did not improve from 0.68721\n","49/49 [==============================] - 12s 251ms/step - loss: 0.4155 - accuracy: 0.9496 - val_loss: 0.7054 - val_accuracy: 0.8674\n","Epoch 10/10\n","49/49 [==============================] - ETA: 0s - loss: 0.3830 - accuracy: 0.9554\n","Epoch 10: val_loss did not improve from 0.68721\n","49/49 [==============================] - 12s 251ms/step - loss: 0.3830 - accuracy: 0.9554 - val_loss: 0.7716 - val_accuracy: 0.8545\n","Process time:  191.73545813560486\n","BiLSTM Accuracy:  0.8657921291624622\n","Input any text: i am happy\n","[[0.00248862 0.02098318 0.9305402  0.01173641 0.0342516 ]]\n","Label:  happy\n"]}],"source":["import numpy as np\n","import pandas as pd \n","import tensorflow as tf\n","import nltk\n","from nltk.stem.porter import PorterStemmer\n","import re\n","import os\n","\n","from tensorflow.keras.layers import Embedding, Dense, GRU, LSTM, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from nltk.corpus import stopwords\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","import time\n","\n","try:\n","    os.remove('/content/drive/MyDrive/final_project/weights_BiLSTM.h5')\n","except OSError:\n","    pass\n","\n","start = time.time()\n","df = pd.read_csv('/content/drive/MyDrive/final_project/Emotion_project.csv') # read the dataset\n","df = df.dropna() # drop columns with NA values\n","X = df.drop('Emotion',axis=1) # input\n","y = df['Emotion'] # output\n","\n","vocab_size = 10000\n","messages = X.copy() # copy of output\n","messages.reset_index(inplace=True)\n","\n","nltk.download('stopwords')\n","# stopwords: frequent words in text.('the', 'and', 'I', etc.) They don't add much meaning to a sentence\n","ps = PorterStemmer()\n","corpus = []\n","for i in range(0, len(messages)):\n","    review = re.sub('[^a-zA-Z]', ' ', messages['Text'][i]) # remove special characters\n","    review = review.lower() # turn into lower case\n","    review = review.split() \n","    review = [ps.stem(word) for word in review if (word == 'not' or word == 'no' or word == 'nor' or not word in stopwords.words('english')) ] # remove stopwords\n","    review = ' '.join(review)\n","    corpus.append(review)\n","\n","onehot_repr = [one_hot(text,vocab_size)for text in corpus] # use one_hot encoding\n","\n","maxlength = 0\n","for x in corpus:\n","    maxlength = max(maxlength, len(x.split(' '))) # find text with max length\n","embedded_docs = pad_sequences(onehot_repr,padding = 'pre',maxlen = maxlength) # pad sequences to the same length\n","\n","X_final = np.array(embedded_docs)\n","label_encoder = preprocessing.LabelEncoder() \n","y_final = label_encoder.fit_transform(y) # encoding the target outputs to integers\n","y_final = np.array(y_final)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.3, random_state=42) # train-test split\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=21)  # train-validation split\n","\n","# create model\n","embedding_vector_features = 100\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_vector_features, input_length = maxlength))\n","# 降維: turn \"One hot representation\" to \"Distributed representation\"(represent the relation between words)\n","model.add(Dropout(0.3)) # randomly sets input units to 0 -> helps prevent overfitting\n","model.add(Bidirectional(LSTM(64))) # 64: dimensionality of the output space (output 維度)\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu', kernel_regularizer = tf.keras.regularizers.l1(0.01))) # output = activation(dot(input, kernel)+bias)\n","model.add(Dropout(0.3))\n","model.add(Dense(5, activation='softmax')) # 5 categories\n","model.compile(loss='sparse_categorical_crossentropy', optimizer= tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\n","model.summary()\n","\n","model_save = ModelCheckpoint('/content/drive/MyDrive/final_project/weights_BiLSTM.h5', save_best_only = True, save_weights_only = True, monitor = 'val_loss', mode = 'min', verbose = 1)\n","# save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved\n","history = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs = 10, batch_size = 256, callbacks = [model_save])\n","#training process, record epoch, loss, accuracy...\n","# batch_size : number of samples per gradient update (一次丟多少資料進去)\n","# epochs : number of iteration ( 1 iteration represents going though whole training data)\n","# validation : check if the data is overfitting\n","model.save('/content/drive/MyDrive/final_project/weights_BiLSTM.h5')\n","model.load_weights('/content/drive/MyDrive/final_project/weights_BiLSTM.h5')\n","\n","end = time.time()\n","print(\"Process time: \",end - start)\n","\n","y_pred=np.argmax(model.predict(X_test), axis=-1) # predict test data\n","print(\"BiLSTM Accuracy: \", accuracy_score(y_test,y_pred)) # calculate accuracy\n"]},{"cell_type":"code","source":["def predict_emotion(stri):\n","    review = re.sub('[^a-zA-Z]', ' ', stri)\n","    review = review.lower()\n","    review = review.split()\n","    review = [ps.stem(word) for word in review if (word == 'not' or word == 'no' or word == 'nor' or not word in stopwords.words('english')) ]\n","    review = ' '.join(review)\n","    onehot_repr = [one_hot(review,vocab_size)] \n","    embed = pad_sequences(onehot_repr,padding='pre',maxlen=maxlength)\n","    predicti = model.predict(embed)\n","    print(predicti)\n","    print(\"Label: \",label_encoder.classes_[np.argmax(predicti)])\n","    #transform(predicti[0],stri)\n","\n","text = input(\"Input any text: \")\n","predict_emotion(text)"],"metadata":{"id":"5Mu0mLSxdUAj","executionInfo":{"status":"ok","timestamp":1655114037798,"user_tz":-480,"elapsed":4684,"user":{"displayName":"陳存佩","userId":"12207295053803165369"}},"outputId":"0035826c-22c3-4955-f143-b5eccdef3c28","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Input any text: i am happy\n","[[0.00248862 0.02098318 0.9305402  0.01173641 0.0342516 ]]\n","Label:  happy\n"]}]}]}