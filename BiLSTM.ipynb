{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BiLSTM.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"970br8_-ktjN"},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","import tensorflow as tf\n","import nltk\n","from nltk.stem.porter import PorterStemmer\n","import re\n","import os\n","\n","from tensorflow.keras.layers import Embedding, Dense, GRU, LSTM, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from nltk.corpus import stopwords\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","import time\n","\n","try:\n","    os.remove('/content/drive/MyDrive/final_project/weights_BiLSTM.h5')\n","except OSError:\n","    pass\n","\n","start = time.time()\n","df = pd.read_csv('/content/drive/MyDrive/final_project/Emotion_project.csv') # read the dataset\n","df = df.dropna() # drop columns with NA values\n","X = df.drop('Emotion',axis=1) # input\n","y = df['Emotion'] # output\n","\n","vocab_size = 10000\n","messages = X.copy() # copy of output\n","messages.reset_index(inplace=True)\n","\n","nltk.download('stopwords')\n","# stopwords: frequent words in text.('the', 'and', 'I', etc.) They don't add much meaning to a sentence\n","ps = PorterStemmer()\n","corpus = []\n","for i in range(0, len(messages)):\n","    review = re.sub('[^a-zA-Z]', ' ', messages['Text'][i]) # remove special characters\n","    review = review.lower() # turn into lower case\n","    review = review.split() \n","    review = [ps.stem(word) for word in review if (word == 'not' or word == 'no' or word == 'nor' or not word in stopwords.words('english')) ] # remove stopwords\n","    review = ' '.join(review)\n","    corpus.append(review)\n","\n","onehot_repr = [one_hot(text,vocab_size)for text in corpus] # use one_hot encoding\n","\n","maxlength = 0\n","for x in corpus:\n","    maxlength = max(maxlength, len(x.split(' '))) # find text with max length\n","embedded_docs = pad_sequences(onehot_repr,padding = 'pre',maxlen = maxlength) # pad sequences to the same length\n","\n","X_final = np.array(embedded_docs)\n","label_encoder = preprocessing.LabelEncoder() \n","y_final = label_encoder.fit_transform(y) # encoding the target outputs to integers\n","y_final = np.array(y_final)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.3, random_state=42) # train-test split\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=21)  # train-validation split\n","\n","# create model\n","embedding_vector_features = 100\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_vector_features, input_length = maxlength))\n","# 降維: turn \"One hot representation\" to \"Distributed representation\"(represent the relation between words)\n","model.add(Dropout(0.3)) # randomly sets input units to 0 -> helps prevent overfitting\n","model.add(Bidirectional(LSTM(64))) # 64: dimensionality of the output space (output 維度)\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu', kernel_regularizer = tf.keras.regularizers.l1(0.01))) # output = activation(dot(input, kernel)+bias)\n","model.add(Dropout(0.3))\n","model.add(Dense(5, activation='softmax')) # 5 categories\n","model.compile(loss='sparse_categorical_crossentropy', optimizer= tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\n","model.summary()\n","\n","model_save = ModelCheckpoint('/content/drive/MyDrive/final_project/weights_BiLSTM.h5', save_best_only = True, save_weights_only = True, monitor = 'val_loss', mode = 'min', verbose = 1)\n","# save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved\n","history = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs = 10, batch_size = 256, callbacks = [model_save])\n","#training process, record epoch, loss, accuracy...\n","# batch_size : number of samples per gradient update (一次丟多少資料進去)\n","# epochs : number of iteration ( 1 iteration represents going though whole training data)\n","# validation : check if the data is overfitting\n","model.save('/content/drive/MyDrive/final_project/weights_BiLSTM.h5')\n","model.load_weights('/content/drive/MyDrive/final_project/weights_BiLSTM.h5')\n","\n","end = time.time()\n","print(\"Process time: \",end - start)\n","\n","y_pred=np.argmax(model.predict(X_test), axis=-1) # predict test data\n","print(\"BiLSTM Accuracy: \", accuracy_score(y_test,y_pred)) # calculate accuracy\n"]},{"cell_type":"code","source":["def predict_emotion(stri):\n","    review = re.sub('[^a-zA-Z]', ' ', stri)\n","    review = review.lower()\n","    review = review.split()\n","    review = [ps.stem(word) for word in review if (word == 'not' or word == 'no' or word == 'nor' or not word in stopwords.words('english')) ]\n","    review = ' '.join(review)\n","    onehot_repr = [one_hot(review,vocab_size)] \n","    embed = pad_sequences(onehot_repr,padding='pre',maxlen=maxlength)\n","    predicti = model.predict(embed)\n","    print(predicti)\n","    print(\"Label: \",label_encoder.classes_[np.argmax(predicti)])\n","    #transform(predicti[0],stri)\n","\n","text = input(\"Input any text: \")\n","predict_emotion(text)"],"metadata":{"id":"5Mu0mLSxdUAj"},"execution_count":null,"outputs":[]}]}